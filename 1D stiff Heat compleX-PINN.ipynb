{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea0198-7f04-49b5-beb7-52f20267c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../Utilities/')\n",
    "\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1) \n",
    "print(torch.cuda.is_available())\n",
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "def figsize(scale, nplots = 1):\n",
    "    fig_width_pt = 390.0                          # Get this from LaTeX using \\the\\textwidth\n",
    "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
    "    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
    "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
    "    fig_height = nplots*fig_width*golden_mean              # height in inches\n",
    "    fig_size = [fig_width,fig_height]\n",
    "    return fig_size\n",
    "\n",
    "def newfig(width, nplots = 1):\n",
    "    fig = plt.figure(figsize=figsize(width, nplots))\n",
    "    ax = fig.add_subplot(111)\n",
    "    return fig, ax\n",
    "\n",
    "def savefig(filename, crop = True):\n",
    "    if crop == True:\n",
    "#        plt.savefig('{}.pgf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
    "        plt.savefig('{}.pdf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
    "        plt.savefig('{}.eps'.format(filename), bbox_inches='tight', pad_inches=0)\n",
    "    else:\n",
    "#        plt.savefig('{}.pgf'.format(filename))\n",
    "        plt.savefig('{}.pdf'.format(filename))\n",
    "        plt.savefig('{}.eps'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc291033-057f-44a4-81d3-cd9b55a46117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(u, x):\n",
    "    \"\"\" Get grad \"\"\"\n",
    "    gradient = torch.autograd.grad(\n",
    "        u, x,\n",
    "        grad_outputs=torch.ones_like(u),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763ef52-bd45-4057-8739-c5561daba9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CustomActivation(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(CustomActivation, self).__init__()\n",
    "        # Initialize trainable parameters alpha1 and d per neuron\n",
    "        self.lambda1 = nn.Parameter(0.1*torch.ones(num_features))\n",
    "        self.lambda2 = nn.Parameter(0.1*torch.ones(num_features))\n",
    "        self.d = nn.Parameter(0.1*torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the custom activation function\n",
    "\n",
    "        denominator = x**2 + self.d**2\n",
    "        term1 = self.lambda1 * x / denominator\n",
    "        term2 = self.lambda2 / denominator\n",
    "        return term1 + term2\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, layers, activation_type='per_neuron'):\n",
    "        super(DNN, self).__init__()\n",
    "        self.depth = len(layers) - 1\n",
    "        layer_list = []\n",
    "        \n",
    "        for i in range(self.depth - 1):\n",
    "            # Add linear layer\n",
    "            layer_list.append(('layer_%d' % i, nn.Linear(layers[i], layers[i+1])))\n",
    "            \n",
    "            # Add activation layer with per-neuron parameters\n",
    "\n",
    "            layer_list.append(('activation_%d' % i, CustomActivation(layers[i+1])))\n",
    "\n",
    "        # Add the final output layer without activation\n",
    "        layer_list.append(('layer_%d' % (self.depth - 1), nn.Linear(layers[-2], layers[-1])))\n",
    "        \n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        self.layers = nn.Sequential(layerDict)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "class CustomLRDecayScheduler:\n",
    "    def __init__(self, optimizer, lr0, decay_rate, decay_step, lr_min):\n",
    "        self.optimizer = optimizer\n",
    "        self.lr0 = lr0\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_step = decay_step\n",
    "        self.lr_min = lr_min\n",
    "        self.optim_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.optim_step += 50\n",
    "        lr = self.lr0 * (self.decay_rate ** (self.optim_step / self.decay_step))\n",
    "        # 检查学习率是否低于 lr_min\n",
    "        if lr < self.lr_min:\n",
    "            lr = self.lr_min\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return lr\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.dnn.parameters() if p.requires_grad)\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        return {\n",
    "            \"allocated\": torch.cuda.memory_allocated() / 1024 ** 2,  # MB\n",
    "            \"cached\": torch.cuda.memory_reserved() / 1024 ** 2  # MB\n",
    "        }\n",
    "    return {\"error\": \"No GPU available\"}\n",
    "\n",
    "def estimate_cpu_memory(model):\n",
    "    param_size = 0\n",
    "    for param in model.dnn.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    buffer_size = 0\n",
    "    for buffer in model.dnn.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    return param_size + buffer_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15a131-9424-414d-b29f-1a63a0ad39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN():\n",
    "    \"\"\" PINN Class \"\"\"\n",
    "    \n",
    "    def __init__(self, X_f, a, layers):\n",
    "         # data\n",
    "        self.x_f1 = X_f[:, 0:1].clone().detach().requires_grad_(True).float().to(device)\n",
    "        self.t_f = X_f[:, 1:2].clone().detach().requires_grad_(True).float().to(device)\n",
    "        self.a = a\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        self.dnn = DNN(layers).to(device)\n",
    "\n",
    "       # Modified optimizer setup\n",
    "        self.optimizer = torch.optim.Adam(self.dnn.parameters(), lr=5e-3, betas=(0.9, 0.99))\n",
    "\n",
    "\n",
    "        self.scheduler = CustomLRDecayScheduler(self.optimizer, lr0=5e-3, decay_rate=0.75, decay_step=1000, lr_min = 1e-5)\n",
    "        \n",
    "        self.iter = 0\n",
    "        self.l2_errors = []\n",
    "        self.l_inf_errors = []\n",
    "        self.MSE_errors = []\n",
    "        self.time_list = [] \n",
    "        self.total_time = 0 \n",
    "\n",
    "        self.rsum = 0\n",
    "        self.eta = 0.001\n",
    "        self.gamma = 0.999\n",
    "\n",
    "    def net_u(self, x1,t):\n",
    "        u = self.dnn(torch.cat([x1, t], dim=1))\n",
    "        u_transform = u * x1 * (1 - x1) * t + 1*torch.sin(1*np.pi*x1) + 2*torch.sin(self.a*np.pi*x1)\n",
    "        return u_transform\n",
    "\n",
    "    def net_f(self, x, t):\n",
    "        \"\"\"\n",
    "        Residual for:\n",
    "            u_t = 0.5 * u_xx + f(x,t)\n",
    "        with\n",
    "            f(x,t) = -3 * (3*pi)**2 * sin(3*pi*x) * exp(-2*(3*pi)**2 * t)\n",
    "        \"\"\"\n",
    "        # Forward\n",
    "        u = self.net_u(x, t)\n",
    "\n",
    "        # First derivatives\n",
    "        u_x = grad(u, x)\n",
    "        u_t = grad(u, t)\n",
    "\n",
    "        # Second derivative in x\n",
    "        u_xx = grad(u_x, x)\n",
    "\n",
    "        # Forcing term f(x,t)\n",
    "        a = self.a\n",
    "        D = 0.5 \n",
    "        f_forcing = -3.0 * (a * np.pi) ** 2 * torch.sin(a * np.pi * x) * torch.exp(-2.0 * (a * np.pi) ** 2 * t)\n",
    "\n",
    "        f_res = u_t - D * u_xx - f_forcing\n",
    "        return f_res\n",
    "\n",
    "    def loss_func(self):\n",
    "        \"\"\" Loss function \"\"\"\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        f_pred = self.net_f(self.x_f1, self.t_f)\n",
    "        loss_f = torch.mean((f_pred)**2)\n",
    "\n",
    "        # Loss calculation\n",
    "        loss = loss_f\n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        if self.iter % 2000 == 0:\n",
    "            print(\n",
    "                'Iter %d, Loss_f: %.5e' % (self.iter, loss_f.item())\n",
    "            )\n",
    "        return loss\n",
    "\n",
    "    def compute_errors(self, X_star):\n",
    "        \"\"\"Compute L2, L_inf, and MSE errors using PyTorch (no numpy).\"\"\"\n",
    "        with torch.no_grad():\n",
    "            u_pred = self.predict(X_star)  # Shape: [N, 1]\n",
    "            X = X_star[:, 0:1]\n",
    "            T = X_star[:, 1:2]\n",
    " \n",
    "            U_exact = np.sin(1*np.pi*X) * np.exp(-0.5*(np.pi**2)*T) + 2*np.sin(self.a*np.pi*X) * np.exp(-2*((self.a*np.pi)**2)*T)\n",
    "            u_exact = torch.tensor(U_exact, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # L2 Error (using PyTorch)\n",
    "            l2_error = torch.linalg.norm(u_pred - u_exact) / torch.linalg.norm(u_exact)\n",
    "            \n",
    "            # L_inf Error\n",
    "            l_inf_error = torch.max(torch.abs(u_pred - u_exact))\n",
    "            \n",
    "            # MSE\n",
    "            mse = torch.mean((u_pred - u_exact) ** 2)\n",
    "            \n",
    "            self.l2_errors.append(l2_error.item())\n",
    "            self.l_inf_errors.append(l_inf_error.item())\n",
    "            self.MSE_errors.append(mse.item())\n",
    "            \n",
    "        return l2_error.item(), l_inf_error.item(), mse.item()\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        self.dnn.train()\n",
    "        self.loss = []\n",
    "        print(f'We are using {type(self.optimizer).__name__} optimizer for optimizer1')  \n",
    "        start_time = time.time()\n",
    "        for i in range(num_epochs):\n",
    "            loss = self.loss_func()\n",
    "            self.optimizer.step()\n",
    "            if (i+1) % 50 == 0:\n",
    "                self.scheduler.step()\n",
    "\n",
    "\n",
    "            if i % 100 == 0 and i > 0:  # 检查是否是每 100 次迭代\n",
    "                end_time = time.time()  # 记录结束时间\n",
    "                elapsed_time = end_time - start_time  # 计算这 100 次迭代所花费的时间\n",
    "                self.total_time += elapsed_time  # 累加总时间\n",
    "                self.time_list.append(self.total_time)  # 将总时间添加到时间列表\n",
    "                start_time = time.time()  # 重置开始时间\n",
    "                \n",
    "            if i % 100 == 0:  # Check if the current iteration is a multiple of 100\n",
    "                l2, l_inf, mse = self.compute_errors(X_star)\n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                print(f\"Adam Epoch {i}: L2={l2:.3e}, Linf={l_inf:.3e}, MSE={mse:.3e}, Learning Rate: {current_lr:e}\")\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        x1 = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.dnn.eval()\n",
    "        u = self.net_u(x1, t)\n",
    "        return u.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43da4b4-b52a-41a0-a94b-9239b0d57783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import qmc\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    N_f = 20000\n",
    "    N_test = 90000\n",
    "    layers = [2, 200, 1]\n",
    "    a = 3\n",
    "\n",
    "    # Define domain bounds for 5D (adjust to your problem)\n",
    "    l_bounds = [0, 0]  # Lower bounds for each dimension\n",
    "    u_bounds = [1, 1]  # Upper bounds for each dimension\n",
    "\n",
    "    # Use Latin Hypercube Sampling (LHS) for 5D points\n",
    "    sampler = qmc.LatinHypercube(d=len(l_bounds))  # d=5\n",
    "    \n",
    "    # Generate training points\n",
    "    X_f_train = sampler.random(n=N_f)\n",
    "    X_f_train = qmc.scale(X_f_train, l_bounds, u_bounds).astype(np.float32)  # Fix: Use l_bounds/u_bounds\n",
    "    \n",
    "    # Generate testing points\n",
    "    X_star = sampler.random(n=N_test)\n",
    "    X_star = qmc.scale(X_star, l_bounds, u_bounds).astype(np.float32)\n",
    "    \n",
    "    X_f_new = torch.from_numpy(X_f_train).float()\n",
    "model = PINN(X_f_new, a, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a93f73-bfa4-42cb-9e7a-ed90ad858937",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.train(num_epochs= 30001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e35dd-d5e3-49b5-85a1-8a1736e855f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Grids\n",
    "x_f = np.linspace(0, 1, 300)\n",
    "y_f = np.linspace(0, 1, 300)  # time grid\n",
    "X, T = np.meshgrid(x_f, y_f)\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "\n",
    "# Model prediction on the flattened grid\n",
    "u_pred = model.predict(X_star).cpu().detach().numpy()           # shape (N,1) or (N,)\n",
    "u_pred = np.asarray(u_pred).reshape(-1, 1)\n",
    "# If it's a torch tensor, you did: u_pred = u_pred.detach().cpu().numpy()\n",
    "\n",
    "# Exact solution\n",
    "U_exact = (np.sin(1*np.pi*X) * np.exp(-0.5*(np.pi**2)*T)\n",
    "           + 2*np.sin(3*np.pi*X) * np.exp(-2*((3*np.pi)**2)*T))\n",
    "u_exact = U_exact.reshape(-1, 1)\n",
    "\n",
    "# L2 relative error on the flattened grid\n",
    "norm_diff = np.linalg.norm(u_exact - u_pred)\n",
    "norm_star = np.linalg.norm(u_exact)\n",
    "error_u = norm_diff / norm_star\n",
    "print('L2 Error: %e' % (error_u))\n",
    "\n",
    "# Reshape prediction back to grid for visualization\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "Error = np.abs(U_exact - U_pred)\n",
    "\n",
    "# Pick the column closest to x = 0.5\n",
    "x0 = 0.75\n",
    "ix = np.argmin(np.abs(x_f - x0))\n",
    "\n",
    "# Extract time vector and the two slices\n",
    "t_slice = y_f                                # same as T[:, ix]\n",
    "u_exact_slice = U_exact[:, ix]               # exact u(t) at x ~ 0.5\n",
    "u_pred_slice = U_pred[:, ix]                 # predicted u(t) at x ~ 0.5\n",
    "\n",
    "# Optional: handle any NaNs from interpolation at edges\n",
    "mask = ~np.isnan(u_pred_slice)\n",
    "t_plot = t_slice[mask]\n",
    "u_exact_plot = u_exact_slice[mask]\n",
    "u_pred_plot = u_pred_slice[mask]\n",
    "\n",
    "# Plot time slice at x ≈ 0.5\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(t_plot, u_exact_plot, 'k-', lw=2, label='Exact solution at x=0.5')\n",
    "plt.plot(t_plot, u_pred_plot, 'r--', lw=2, label='compleX-PINN Prediction at x=0.5')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('u(x=0.5, t)')\n",
    "plt.title(r'Time slice of compleX-PINN at $x = 0.5$')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847cfe1c-7e80-4179-9421-f50091c0874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_f = np.linspace(0 , 1, 300)\n",
    "y_f = np.linspace(0 , 1, 300)\n",
    "    \n",
    "X, T = np.meshgrid(x_f, y_f)\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_pred = model.predict(X_star)\n",
    "u_pred = u_pred.detach().cpu().numpy()\n",
    "# Compute the exact solution\n",
    "\n",
    "U_exact = 1*np.sin(1*np.pi*X) * np.exp(-0.5*(np.pi**2)*T) + 2*np.sin(3*np.pi*X) * np.exp(-2*((3*np.pi)**2)*T)\n",
    "u_exact = U_exact.reshape(-1 ,1)\n",
    "norm_diff = np.sqrt(np.sum((u_exact  - u_pred)**2))\n",
    "norm_star = np.sqrt(np.sum((u_exact)**2))\n",
    "error_u = norm_diff / norm_star\n",
    "print('L2 Error: %e' % (error_u))                 \n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "Error = np.abs(U_exact- U_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d78860-6a71-43e4-b731-fdf4a5a70dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.tri as tri\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sn\n",
    "X_star_total = np.vstack((X_star))\n",
    "x_total = X_star_total[:, 0:1]\n",
    "y_total = X_star_total[:, 1:2]\n",
    "\n",
    "# Create a Triangulation instance\n",
    "triang_total = tri.Triangulation(x_total.flatten(), y_total.flatten())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 7))\n",
    "\n",
    "# Plot u_pred using tricontourf\n",
    "contour = ax.tricontourf(triang_total, u_exact.flatten(), 100, cmap='jet')\n",
    "tcbar = fig.colorbar(contour)\n",
    "tcbar.ax.tick_params(labelsize=18)\n",
    "ax.set_xlabel('$x$', fontsize = 18)\n",
    "ax.set_ylabel('$y$', fontsize = 18)\n",
    "ax.tick_params(axis=\"x\", labelsize = 20)\n",
    "ax.tick_params(axis=\"y\", labelsize = 20)  \n",
    "#ax.xaxis.set_major_locator(ticker.MultipleLocator(0.5)) \n",
    "ax.set_title('Exact solution', fontsize = 24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07542908-a17e-4be9-94b5-646bea6e6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.tri as tri\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sn\n",
    "u_pred = np.concatenate([u_pred])\n",
    "\n",
    "X_star_total = np.vstack((X_star))\n",
    "x_total = X_star_total[:, 0:1]\n",
    "y_total = X_star_total[:, 1:2]\n",
    "\n",
    "# Create a Triangulation instance\n",
    "triang_total = tri.Triangulation(x_total.flatten(), y_total.flatten())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 7))\n",
    "\n",
    "# Plot u_pred using tricontourf\n",
    "contour = ax.tricontourf(triang_total, u_pred.flatten(), 100, cmap='jet')\n",
    "tcbar = fig.colorbar(contour)\n",
    "tcbar.ax.tick_params(labelsize=18)\n",
    "ax.set_xlabel('$x$', fontsize = 18)\n",
    "ax.set_ylabel('$t$', fontsize = 18)\n",
    "ax.tick_params(axis=\"x\", labelsize = 20)\n",
    "ax.tick_params(axis=\"y\", labelsize = 20)  \n",
    "#ax.xaxis.set_major_locator(ticker.MultipleLocator(0.5)) \n",
    "ax.set_title('compleX-PINN Predicted solution', fontsize = 24)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot the error\n",
    "fig, ax = plt.subplots(figsize=(11, 7))\n",
    "contour = ax.contourf(X, T, Error, 100, cmap='jet')\n",
    "tcbar = fig.colorbar(contour)\n",
    "tcbar.ax.tick_params(labelsize=18)\n",
    "ax.set_xlabel('$x$', fontsize = 18)\n",
    "ax.set_ylabel('$t$', fontsize = 18)\n",
    "ax.tick_params(axis=\"x\", labelsize = 20)\n",
    "ax.tick_params(axis=\"y\", labelsize = 20)  \n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_powerlimits((-2, 2))\n",
    "tcbar.ax.yaxis.set_major_formatter(formatter)\n",
    "# Make the scientific notation label larger\n",
    "tcbar.ax.yaxis.offsetText.set_fontsize(20)\n",
    "#ax.xaxis.set_major_locator(ticker.MultipleLocator(0.25))\n",
    "#ax.yaxis.set_major_locator(ticker.MultipleLocator(0.25))\n",
    "ax.set_title('compleX-PINN Point-wise Error', fontsize = 24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757dab2-3ca9-4f9c-8e7b-0bd418e69045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.7 (PyTorch)",
   "language": "python",
   "name": "python312_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
